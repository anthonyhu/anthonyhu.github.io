---
layout: post
title: The path from AlphaGo to muZero
permalink: muzero
comments: true
github: https://github.com/anthonyhu/
---
<p align='center'><img src='/img/muzero.jpg' alt='muzero'/></p>

muZero, the latest research project by DeepMind, made a breakthrough on Atari, Go, Chess and Shogi using
model-based reinforcement learning.

### Content
1. [AlphaGo](#1-alphago)
2. [AlphaGo Zero](#2-alphago-zero)
3. [AlphaZero](#3-alphazero)
4. [muZero](#4-muzero)

## 1. AlphaGo
AlphaGo achieved a world's first by beating the Go champion Lee Sedol, a feat the research community thought was decades 
away due to the complexity of the board game. It uses two networks: the policy network (that predicts what are the best
action given the current board position) and the value network (evaluating how likely you are to win the game).

### Network architecture 
#### Policy network
Given a state $s$ of the board, the policy network predicts the most promising action to take $p(a|s)$. The input $s$
(the 19x19 Go board) goes through 14 convolutional layers (192 filters for the hidden layers). It is first trained on
human expert data (policy $p_{\sigma}$), to predict the next move of the human player, by maximising the likelihood 
of the human moves:

$$ \Delta \sigma \propto \frac{\partial \text{log}~p_{\sigma}(a|s)}{\partial \sigma} $$

Then, the policy network is 
trained with reinforcement
learning ($p_{\rho}$) through self-play. It plays against different versions of itself to have a bigger pool of 
candidates
 to 
train with. The policy network is now optimised to win the game, instead of simply trained to predict the next move. 
It is optimised with policy gradient, that is to say by encouraging the moves that lead to victory, and penalising 
the moves that made the player lose:

$$ \Delta \rho \propto \frac{\partial \text{log}~p_{\rho}(a|s)}{\partial \rho} z_t$$

with $z_t$ the terminal reward at the end of the game, equal to +1 if the current player is winning, else -1.


#### Value network
The value network how good the position is. It predicts a single scalar and was trained on 30M different board positions
from 30M unique games from self-play. Training on the human expert data lead to overfitting as the network would 
learn to memorise each game. 

### Search algorithm
AlphaGo uses a variant on the Monte Carl Tree Search (MCTS) algorithm, combining the policy network to explore 
promising moves, and the value network, to evaluate the goodness of a position instead of expanding the tree until 
terminal reward. This search algorithm leads to superhuman performance with only 2s per move. 

Each edge of the search tree $(s, a)$ stores an action value $Q(s, a)$, visit count $N(s,a)$ and prior probability $P
(s, a)$. At each step, the algorithm picks the best action:

$$a_t =  \underset{a}{\text{argmax}}~Q(s_t, a) + u(s_t, a)$$

in order to maximise the action value and a bonus

$$u(s, a) \propto \frac{P(s, a)}{1 + N(s,a)}$$

proportional to the prior probability but decays with the count number to encourage exploration. 

The prior probability $P(s, a)$ is computed with the policy network, the action value $Q(s, a)$ and count $N(s,a)$ 
are initialised to zero and updated with the $n$ MCTS simulations:

$$
\begin{align}
N(s, a) &= \sum_{i=1}^n \mathbb{1}(s, a, i)\\
Q(s, a) &= \frac{1}{N(s, a)} \sum_{i=1}^n \mathbb{1}(s, a, i)V(s_L^i)
\end{align}
$$

with $\mathbb{1}(s, a, i)$ indicating whether edge $(s, a)$ was visited in simulation $i$. $V(s_L^i)$ is the value of
 the leaf state $s_L$ (i.e. a state that does not contain any stored edge $(s, a)$), evaluated with the value network.
 
 AlphaGo picks the action that was chosen the most in the $n$ simulations. 

## 2. AlphaGo Zero

## 3. AlphaZero

## 4. muZero