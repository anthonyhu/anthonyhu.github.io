---
layout: post
title: The path from AlphaGo to MuZero
permalink: muzero
comments: true
github: https://github.com/anthonyhu/
---
<p align='center'><img src='/img/muzero.jpg' alt='muzero'/></p>

MuZero, the latest research project by DeepMind, made a breakthrough on Atari, Go, Chess and Shogi using
model-based reinforcement learning.

### Content
1. [AlphaGo](#1-alphago)
2. [AlphaGo Zero](#2-alphago-zero)
3. [AlphaZero](#3-alphazero)
4. [MuZero](#4-muzero)

## 1. AlphaGo
AlphaGo achieved a world's first by beating the Go champion Lee Sedol, a feat the research community thought was decades 
away due to the complexity of the board game. It uses two networks: the policy network (that predicts what are the best
action given the current board position) and the value network (evaluating how likely you are to win the game).

### Network architecture 
#### Policy network
Given a state $s$ of the board, the policy network predicts the most promising action to take $p(a|s)$. The input $s$
(the 19x19 Go board) goes through 14 convolutional layers (192 filters for the hidden layers). It is first trained on
human expert data (policy $p_{\sigma}$), to predict the next move of the human player, by maximising the likelihood 
of the human moves:

$$ \Delta \sigma \propto \frac{\partial \text{log}~p_{\sigma}(a|s)}{\partial \sigma} $$

Then, the policy network is 
trained with reinforcement
learning ($p_{\rho}$) through self-play. It plays against different versions of itself to have a bigger pool of 
candidates
 to 
train with. The policy network is now optimised to win the game, instead of simply trained to predict the next move. 
It is optimised with policy gradient, that is to say by encouraging the moves that lead to victory, and penalising 
the moves that made the player lose:

$$ \Delta \rho \propto \frac{\partial \text{log}~p_{\rho}(a|s)}{\partial \rho} z_t$$

with $z_t$ the terminal reward at the end of the game, equal to +1 if the current player is winning, else -1.


#### Value network
The value network how good the position is. It predicts a single scalar and was trained on 30M different board positions
from 30M unique games from self-play. Training on the human expert data lead to overfitting as the network would 
learn to memorise each game. 

### Search algorithm
AlphaGo uses a variant on the Monte Carl Tree Search (MCTS) algorithm, combining the policy network to explore 
promising moves, and the value network, to evaluate the goodness of a position instead of expanding the tree until 
terminal reward. This search algorithm leads to superhuman performance with only 2s per move. 

Each edge of the search tree $(s, a)$ stores an action value $Q(s, a)$, visit count $N(s,a)$ and prior probability $P
(s, a)$. At each step, the algorithm picks the best action:

$$a_t =  \underset{a}{\text{argmax}}~Q(s_t, a) + u(s_t, a)$$

in order to maximise the action value and a bonus

$$u(s, a) \propto \frac{P(s, a)}{1 + N(s,a)}$$

proportional to the prior probability but decays with the count number to encourage exploration. 

The prior probability $P(s, a)$ is computed with the policy network, the action value $Q(s, a)$ and count $N(s,a)$ 
are initialised to zero and updated with the $n$ MCTS simulations:

$$
\begin{align}
N(s, a) &= \sum_{i=1}^n \mathbb{1}(s, a, i)\\
Q(s, a) &= \frac{1}{N(s, a)} \sum_{i=1}^n \mathbb{1}(s, a, i)V(s_L^i)
\end{align}
$$

with $\mathbb{1}(s, a, i)$ indicating whether edge $(s, a)$ was visited in simulation $i$. $V(s_L^i)$ is the value of
 the leaf state $s_L$ (i.e. a state that does not contain any stored edge $(s, a)$), evaluated with the value network.
 
 AlphaGo picks the action that was chosen the most in the $n$ simulations. 

## 2. AlphaGo Zero
Without any human knowledge, AlphaGo Zero learned to play Go solely from self-play and beat its previous version, 
AlphaGo, 100 to 0. The three key differences are:

1. No human data, AlphaGo Zero was not trained with supervised learning but only with reinforcement learning.
2. No prior knowledge of the game: the input of the network is the raw board + history.
3. Shared representation of policy and value networks by using a single neural net.

AlphaGo Zero trains a unique neural network $f_{\theta}$ that outputs a vector of probability $p_a$ indicating the 
most promising actions, and a scalar $v$ indicating the strength of the current position. Initialised from random 
weights $\theta_0$, each iteration of self-play updates the parameters of the network by minimising the cross-entropy
 loss between the predicted moves $p_a$ and the MCTS recommended moves, and the L2 distance between the predicted 
 value $v$ and the actual outcome of the game $z$. 
 
The MCTS algorithm outputs a vector of probability of lookahead search proportional to $N(s,a)^{\frac{1}{\tau}}$ 
($\tau$ being the temperature parameter. $\tau=1$ corresponds to picking moves proportional to their count, and $\tau 
\to 0$ corresponds to deterministically selecting the most visited move).

This results in a much stronger player compared to the previous AlphaGo (5,185 vs. 3,739 Elo rating). 

## 3. AlphaZero
To further demonstrate that DeepMind's method is a general-purpose algorithm, they built AlphaZero, mastering Chess, 
Shogi and Go without any human knowledge or handcrafted features/rules. AlphaZero uses deep neural networks to learn 
a representation of the board, a general-purpose reinforcement learning algorithm and a general-purpose tree search 
algorithm. 

Using the same neural network architecture and learning procedure, AlphaZero achieves superhuman performance and 
beats all previous chess (Stockfish) or Shogi (Elmo) state-of-the-art computer methods.  

## 4. MuZero
AlphaZero works on perfect information games with a perfect simulator. That restricts the scope of applicability of 
the algorithm as it is very rare to have access to a perfect simulator. MuZero extends AlphaZero by learning a model 
of the environment so that the simulator is no longer necessary in the MCTS. 

AlphaZero uses game-specific knowledge in: the state transition, the plausible action, and the terminal state. MuZero
 learns a dynamics model for the state transition, uses the policy to pick plausible actions (the policy quickly 
 learns to discard impossible actions), and also learns to predict the reward (the reward is constant equal to the 
 terminal state reward when the game finishes).
 
 
The main change is the dynamics model $g$, that predicts the next state $s_{t+1}$ given the current state $s_t$ and 
action $a_{t+1}$.
 
$$s_{t+1} = g_{\theta}(s_t, a_{t+1})$$

The dynamics model (which is deterministic) is used in the MCTS to find the optimal action using the lookahead search
.

The state is not the history of RGB pixels, but an abstract representation. This representation has no constraint 
other than the signal from the reward and the correct action taken. It can be structured freely internally.

$$s_t^0 = h_{\theta}(o_1, ..., o_t)$$

The dynamics network is convolutional.

And like before, a neural network predicts the value and policy given the current state:

$$p_t^k, v_t^k =f_{\theta}(s_t^0)$$

The three equations above can be represented with a unique model $\mu_{\theta}$:

$$p_t^k, v_t^k, r_t^k = \mu_{\theta}(o_1, ..., o_t, a_{t+1}, ..., a_{t+k})$$


The target value function is:

$$z_t = \begin{cases} u_T & \mbox{for board games} \\ u_{t+1} + \gamma u_{t+2} + ... + \gamma^{n-1} 
u_{t+n} & \mbox{for general MDPs} \end{cases}$$

The total loss for MuZero parametetrised with $\theta$ is: 

$$l_t(\theta) = \sum_{k=0}^K \left[ l^r(u_{t+k}, r_t^k) + l^v(z_{t+k}, v_t^k) + l^p(\pi_{t+k}, p_t^k) \right] + 
c\Vert \theta \Vert^2$$


  

Less prone to overfitting because learns the mechanisms of the environment. 

